### What is "featurePruner" and what is "pruner"?
`featurePruner` is a package. `pruner` is the main function in featurePruner that finds the most predictive features for your modelling tasks in your data. 
In the documentation, we try to use pruner when talking about the specific function and featurePruner when talking more generally about the package. 

### When should you use featurePruner?
Whenever you have a dataset which you suspect to have has noisy, non-value add variables in the dataset, featurePruner can help you sort the variables that are important & predictive. featurePruner was built keeping in mind that datasets in real world have really high-correlated & noisy variables. But this package can also be used for datasets without such issues. Secondly, featurePruner can be used for both classification & regression modelling tasks. 

### What are pruning_intensity levels and how do you choose the right one?
On a high-level, the `pruning_intensity` levels tells the pruner function how strict or aggressive you want the package to be when returning recommendation of predictive variables. The five `pruning_intensity` levels - 1, 2, 3, 4, and 5 - with 1 being the least strict and 5 being the most strict. featurePruner can be run at any of these five levels by setting the `pruning_intensity` argument of the `pruner` function to that number, or all five at once by setting the `pruning_intensity = None`. The ideal value you choose depends on how many variables you have in your data & how many variables you want to keep in your model. Note that there is negligible computation cost for running this package at all 5 pruning_intensity levels, so feel free to set it to None.


### It looks like featurePruner uses random permutation to help make decisions. Can I have more explanation on that logic?
featurePruner randomly permutes each variable and then sees how the model performs with that variable permuted. Meaning, the variables in your data are randomly re-ordered one at a time to check the difference in importance it causes. This is a useful & model agnostic way to measure variable importance (permutation importance) and featurePruner pushes this further by measuring correlation of each variable in the prediction space. The results obviously can vary slightly based on the random order for each variable. If this variance is unpalatable, running the permutations multiple times and then averaging the results will reduce the volatility. This is controlled through the `permutation_rounds` argument to `pruner`. 1 is the default. For small datasets, set this to as many runs as you want. For large datasets, start with 1 & slowly increase it until the computation time is worth waiting for.

### featurePruner is taking too long to complete its analysis. What can I do?
* Two parts of the `pruner` function can take a really long time. The first is hyperparameter tuning if `auto_tune_missing_parameters = True`. Setting this to `False` will completely skip that part of the process, albiet at the risk of using the defalt LightGBM parameters mentioned in the utility.py file. Otherwise, you can control how many hyperparameter runs occur through the `total_tuning_runs` arguments. The lower the value of total_tuning_runs, the less hyperparmeter tuning occurs. 
* The other part of `pruner` that takes a really long time is permutation importance and creating the correlation matrix. The length of time here is a function of several things (1) how complex the model is (i.e. how many trees and how deep), (2) how many variables are in the dataset that haven't been excluded after the Primary model (3) how many rows are in the validation dataset. The best way to control (1) is by passing in a higher `learning_rate` than the default so the model grows less trees. The best way to control (2) is to set a larger `min_gain_percent_for_pruning` to discard more variables after the Primary model. And the best way to control (3) is to specify a smaller validation set either through `validation_percent` if a random validation set is requested or `validation_column` if a customized validation set is passed. On top of all that, LightGBM is faster when you have more cores, so all else being equal, running this on a computer with more cores should speed things along too.

### How does featurePruner perform with highly correlated variables? 
We conducted a study with synthetic data which has highly correlated variables & we found featurePruner to be very good at identifying such variables & removing them. 

### What is the rule set that vaperiser employs to remove or retain variables at each aggression level?
The source code to this particular logic is present in  set is located in helper_functions.py in the `variables_by_pruning_intensity()` function. Within this function, the line that begins with `best_cluster_retained_variables = ...` is where the logic is encoded.

featurePruner retains the most important variable from a cluster group if and only if it's importance (either permutation or gain) is greater than `apriori_importance` which itself varies based on the number variables used by the main model and a scalar multiplier determined by the pruning_intensity level. Lower pruning_intensity levels have more cluster groups and lower constant multipliers such that there are more "most important variable from a cluster group" and more of these will clear the `apriori_importance` hurdle. 

Next featurePruner also retains strong variables no matter if they are most important in the cluster group or not. For situations where you have multiple strong variables in a cluster group, many times we don't only want to just keep one. Strong variables are those that have either permutation or gain importance greater than `apriori_importance` multiplied by a scalar that varies by pruning_intensity level.. 
Finally, we filter out variables that have a negative permutation importance since they can't be relied upon to generalize well to future unseen data.